## Linear Regression with Multiple variables

We have seen how to solve linear Regression model. Linear Regression is very less useful in solving real world problems
because our hypothesis depends upon many parameters. For example, the rent of a house depends on many factors like the neighborhood it is in, size of it, no.of rooms, attached facilities, distance of nearest station from it, distance of nearest shopping area from it, etc. To understand how to use Linear Regression with Multiple variables, we have to see Multivariate Linear Regression.

The multivariable form of the hypothesis function accommodating these multiple features is as follows:

h<sub>θ</sub>(x)=θ<sub>0</sub>+θ<sub>1</sub>x<sub>1</sub>+θ<sub>2</sub>x<sub>2</sub>+θ<sub>3</sub>x<sub>3</sub>+⋯+θ<sub>n</sub>x<sub>n</sub>

In order to develop intuition about this function, we can think about θ<sub>0</sub> as the basic price of a house, θ<sub>1</sub> as the price per square meter, θ<sub>2</sub> as the price per floor, etc. x<sub>1</sub> will be the number of square meters in the house, x<sub>2</sub> the number of floors, etc.

Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:

![vectorized representation](/pics/multivariate/vector.png)

This is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more.

So we have now multivariate hypothesis function. Our next step is to estimate the parametes in the hypothesis function using Gradient Descent. Here, we just have to repeat Gradient Descent algorithms for 'n' features or parameters.

![Gradient Descent](/pics/multivariate/gradient-descent.png)

The following image compares gradient descent with one variable to gradient descent with multiple variables:

![one-variable vs. Multiple-Variable](/pics/multivariate/comparision.png)

### How to improve convergence of cost function using gradients descent?
Generally, when it comes to multivariate linear regression, we don't throw in all the independent variables at a time and start using gradient descent for  minimizing the error function.Preprocessing of Datasets containing multiple features is necessary.we can make gradient descent work well and faster by following ideas:
     1. Features Scaling
     2. mean normalization
#### Features Scaling
  Def. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1.

  At first, we should make sure that features are on a similar scale or on similar ranges of values.
![Features Scaling](/pics/multivariate/feature-scaling.png)

On above figure, we have given two features where x<sub>1</sub> is the size of house and takes on values between 0 to 2000 and x<sub>2</sub> is the number of bedrooms and takes on values between 1 to 5. Because of very large range between x<sub>1</sub> and x<sub>2</sub>  It turns out that the contours of the cost function J of theta can take on  very very skewed elliptical shape (see blue colored eclipse), and running gradient descent on this cost function will end up taking a long time and can oscilate back and forth and take a long time for convergence or reaching the global minimum(look the red colored path).

 The right side of diagram is much less skewed because we have scaled our features between 0 and 1. The makes ranges of our features so that they are all roughly the same. Ideally:

−1 ≤ x<sub>(i)</sub> ≤ 1

or

−0.5 ≤ x<sub>(i)</sub> ≤ 0.5

These aren't exact requirements; we are only trying to speed things up. The goal is to get all features into roughly one of these ranges, give or take a few.

#### mean normalization
Def. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.

To implement both of these techniques, adjust your input values as shown in this formula:

![feature-scaling](/pics/multivariate/formula.png)

**Note** that dividing by the range, or dividing by the standard deviation, give different results.
