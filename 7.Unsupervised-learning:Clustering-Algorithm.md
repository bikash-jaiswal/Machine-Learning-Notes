## Unsupervised learning

**Let's review again what is Unsupervised Learning?**

In supervised learning our goal was, given a training set, to learn a function *h : X → Y* so that **h(x)** is a “good” predictor for the corresponding value of **y**. For historical reasons, this function h is called a **hypothesis**. But there is no such case in unsupervised learning as training set doesn't have any label with them i.e no Y for X.

![unsupervised](pics/unsupervised/unsupervised.png)
![unsupervised Examples](pics/unsupervised/unsupervised-examples.png)

In the clustering problem we are given an unlabeled data set and we would like to have an algorithm automatically group the data into coherent subsets or into coherent clusters for us. The K Means algorithm is by far the most popular, by far the most widely used clustering algorithm

## K-Mean Clustering Algorithm

## Dimensionality Reduction
There are a couple of different reasons why one might want to do dimensionality reduction.

One is **data compression**, which not only allows us to compress the data and have it therefore use up less computer memory or disk space, but it will also allow us to speed up our learning algorithms.
![Data Compression](pics/unsupervised/data-compression.png)
We may want to reduce the dimension of our features if we have a lot of redundant data.

To do this, we find two highly correlated features, plot them, and make a new line that seems to describe both features accurately. We place all the new features on this single line.

Note: in dimensionality reduction, we are reducing our features rather than our number of examples. Our variable m will stay the same size; n, the number of features each example from x<sup>(1)</sup> to x<sup>(m)</sup> carries, will be reduced.

Another application of Dimensionality Reduction is **Data Visualization**.
It is not easy to visualize data that is more than three dimensions. We can reduce the dimensions of our data to 3 or less in order to plot it.

We need to find new features, z1,z2(and perhaps z3) that can effectively summarize all the other features.

Example: hundreds of features related to a country's economic system may all be combined into one feature that you call "Economic Activity."
![Data vizualization](pics/unsupervised/data-viz.png)
![Data vizualization](pics/unsupervised/data-viz1.png)
![Data vizualization](pics/unsupervised/data-viz2.png)

## Principal Component Analysis
For the problem of dimensionality reduction the most commonly used algorithm is principle components analysis (PCA).

Given two features, x<sub>1</sub> and x<sub>2</sub>, we want to find a single line that effectively describes both features at once. We then map our old features onto this new line to get a new single feature.

The same can be done with three features, where we map them to a plane.

The goal of PCA is to reduce the average of all the distances of every feature to the projection line. This is the projection error.

Reduce from 2d to 1d: find a direction (a vector u<sup>(1)</sup>∈ R<sup>n</sup>) onto which to project the data so as to minimize the projection error.

The more general case is as follows:

Reduce from n-dimension to k-dimension: Find k vectors u<sup>(1)</sup>,u<sup>(2)</sup>, ..... ,u<sup>(k)</sup> onto which to project the data so as to minimize the projection error.

If we are converting from 3d to 2d, we will project our data onto two directions (a plane), so k will be 2.
![PCA](pics/unsupervised/PCA.png)
![PCA](pics/unsupervised/PCA1.png)
### PCA is not a Linear regression
* In linear regression, we are minimizing the squared error from every point to our predictor line. These are vertical distances.
* In PCA, we are minimizing the *shortest distance*, or shortest *orthogonal* distances, to our data points.

More generally, in linear regression we are taking all our examples in x and applying the parameters in Θ to predict y.

In PCA, we are taking a number of features x<sub>1</sub>,x<sub>2</sub>,x<sub>3</sub>, ... ,x<sub>n</sub> and finding a closest common dataset among them. We aren't trying to predict any result and we aren't applying any theta weights to the features.

![PCA](pics/unsupervised/PCA2.png)

## Principal Component Analysis Algorithm

Before we can apply PCA, there is a data pre-processing step we must perform:

![Preprocessing](pics/unsupervised/preprocessing.png)
![Preprocessing](pics/unsupervised/pca-algo.png)
![Preprocessing](pics/unsupervised/pca-algo1.png)
![Preprocessing](pics/unsupervised/pca-algo-summary.png)

## Reconstruction from Compressed Representation

If we use PCA to compress our data, how can we uncompress our data, or go back to our original number of features?
![Reconstructing](pics/unsupervised/reconstruct.png)

*Note that we can only get approximations of our original data*.

*Note*: It turns out that the U matrix has the special property that it is a Unitary Matrix. One of the special properties of a Unitary Matrix is:

U<sup>−1</sup>=U<sup>∗</sup> where the "\*" means "conjugate transpose".

Since we are dealing with real numbers here, this is equivalent to:

U<sup>−1</sup>=U<sup>T</sup> So we could compute the inverse and use that, but it would be a waste of energy and compute cycles.

## Choosing the Number of Principal Components

How do we choose k, also called the number of principal components? Recall that k is the dimension we are reducing to.

![Choosing K](pics/unsupervised/choosing-k.png)

In other words, the squared projection error divided by the total variation should be less than one percent, so that 99% of the variance is retained.

### Algorithm for choosing k

![Algorithm of Choosing K](pics/unsupervised/choosing-k1.png)

## Advice for Applying PCA

The most common use of PCA is to speed up supervised learning.
![Advice](pics/unsupervised/advice.png)
![Advice](pics/unsupervised/advice1.png)
![Advice](pics/unsupervised/advice2.png)
![Advice](pics/unsupervised/advice4.png)
